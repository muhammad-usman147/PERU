# -*- coding: utf-8 -*-
"""ResignationPrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eQYWmpA2r67uSnAe9xC1piv4MLLCebxO
"""

from google.colab import drive
drive.mount('/content/drive')

cd /content/drive/MyDrive/PeruProject

ls

import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

data = pd.read_excel('dataemployees.xlsx')

data.head()

"""## Data Preprocessing"""

#drop unnecessary columns
data.drop(columns=['Age','Education','EmployeeNumber','EducationField'],inplace=True,axis=1)

data['FECHA_DE_CESE'].unique()

data.head()

le = LabelEncoder()

data['Gender'] = le.fit_transform(data['Gender'])
data['Department'] = le.fit_transform(data['Department'])
data['JobRole'] = le.fit_transform(data['JobRole'])
data['MaritalStatus'] = le.fit_transform(data['MaritalStatus'])

data['Ovetime'].fillna('No', inplace=True)
data['Ovetime'] = data['Ovetime'].replace('Si', 'Yes')
data['Ovetime'] = data['Ovetime'].replace(1000, 'No')
data['Ovetime'] = le.fit_transform(data['Ovetime'])

data

data['Resigned'] = data['FECHA_DE_CESE'].apply(lambda x: 'Yes' if pd.notna(x) else 'No')

data.drop(['FECHA_DE_CESE'],inplace=True,axis=1)

data.head()

type(data['FECHA_DE_INGRESO'])

data = data[data['FECHA_DE_INGRESO'] != 'SOLTERO (A)']

# Feature engineering: Extract datetime features
data['FECHA_DE_INGRESO'] = pd.to_datetime(data['FECHA_DE_INGRESO'])
data['joining_year'] = data['FECHA_DE_INGRESO'].dt.year
data['joining_month'] = data['FECHA_DE_INGRESO'].dt.month
data['joining_day'] = data['FECHA_DE_INGRESO'].dt.day

data.head()

data.drop('FECHA_DE_INGRESO',inplace=True,axis=1)

data

"""## Resolve Class Imbalancement"""

counts = data['Resigned'].value_counts()

# Print the counts
print(counts)

df = data

from imblearn.over_sampling import RandomOverSampler

X = data.drop('Resigned',axis=1)
y = data['Resigned']

# Apply oversampling to balance the classes
oversampler = RandomOverSampler(sampling_strategy='auto', random_state=42)
X_resampled, y_resampled = oversampler.fit_resample(X, y)

# Create a new DataFrame with the resampled data
df_resampled = pd.concat([X_resampled, y_resampled], axis=1)

counts = df_resampled['Resigned'].value_counts()

# Print the counts
print(counts)

X = df_resampled.drop('Resigned',axis=1)
y = df_resampled['Resigned']

X

"""## ML model"""

# Split your data into a training and testing set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=9)

# Instantiate the Random Forest Classifier
rf_classifier = RandomForestClassifier(n_estimators=4, random_state=9)

# Train the model on the training data
rf_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred = rf_classifier.predict(X_test)

"""## Results"""

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

from itertools import combinations,product
for comb in list(product( [i*10 for i in range(2,10)],[1,5, 40, 50, 75, 30])):
  n_estimators, max_depth = comb[0],comb[1]

  temp_model = rf_classifier = RandomForestClassifier(n_estimators=n_estimators,
                                                      max_depth=max_depth,
                                                      #min_samples_split=min_samples_split,
                                                      min_impurity_decrease=0.5,
                                                      random_state=9)
  temp_model.fit(X_train, y_train)
  y_pred = temp_model.predict(X_test)
  accuracy = accuracy_score(y_test, y_pred)
  if accuracy < 1.0 and accuracy >.90:
    print(f'Accuracy: {accuracy}')

"""# SMOTE"""

import pandas as pd
from imblearn.over_sampling import SMOTE
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

# Create a synthetic imbalanced dataset (you can replace this with your own dataset)
X, y = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0, n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=42)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the SMOTE object with desired sampling strategy
smote = SMOTE(sampling_strategy='auto', random_state=42)

# Fit and apply SMOTE to the training data
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Now you have a balanced dataset (X_train_resampled, y_train_resampled) that you can use for training your model

# Check the class distribution after applying SMOTE
print(pd.Series(y_train_resampled).value_counts())

# Split your data into a training and testing set
X_train, X_test, y_train, y_test = train_test_split(X_train_resampled, y_train_resampled, test_size=0.8, random_state=9)

# Instantiate the Random Forest Classifier
rf_classifier = RandomForestClassifier(n_estimators=4, random_state=9)

# Train the model on the training data
rf_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred = rf_classifier.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

